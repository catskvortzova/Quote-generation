{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "quotes-with-transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "I_ZE8UqIkLEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a19a6dbc-398a-4c7f-e349-b31bdbff2fec"
      },
      "source": [
        "!pip install youtokentome"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: youtokentome in /usr/local/lib/python3.6/dist-packages (1.0.6)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "aruDMIW4kLEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "import youtokentome as yttm\n",
        "import os\n",
        "from train_utils import train_eval_loop\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xtw1YvqkLEt",
        "colab_type": "text"
      },
      "source": [
        "### Load and preparation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Icl76LrkkLEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quotes = pd.read_csv('../data/quotes-2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_gxlZKtli8C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "30d6be71-271f-4adc-dbbf-272af4d78599"
      },
      "source": [
        "quotes.head(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>quote</th>\n",
              "      <th>author</th>\n",
              "      <th>lenght</th>\n",
              "      <th>c</th>\n",
              "      <th>m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You know you're in love when you can't fall as...</td>\n",
              "      <td>Dr. Seuss</td>\n",
              "      <td>18</td>\n",
              "      <td>dreams</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A friend is someone who knows all about you an...</td>\n",
              "      <td>Elbert Hubbard</td>\n",
              "      <td>13</td>\n",
              "      <td>friendship</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Darkness cannot drive out darkness: only light...</td>\n",
              "      <td>Martin Luther King Jr., A Testament of Hope: T...</td>\n",
              "      <td>20</td>\n",
              "      <td>inspirational</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>We accept the love we think we deserve.</td>\n",
              "      <td>Stephen Chbosky, The Perks of Being a Wallflower</td>\n",
              "      <td>8</td>\n",
              "      <td>inspirational</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It is better to be hated for what you are than...</td>\n",
              "      <td>André Gide, Autumn Leaves</td>\n",
              "      <td>19</td>\n",
              "      <td>life</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               quote  ...   m\n",
              "0  You know you're in love when you can't fall as...  ...  85\n",
              "1  A friend is someone who knows all about you an...  ...  52\n",
              "2  Darkness cannot drive out darkness: only light...  ...  91\n",
              "3            We accept the love we think we deserve.  ...  32\n",
              "4  It is better to be hated for what you are than...  ...  62\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9BE9wc0lgNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phrases = list(quotes.quote.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Btf92DHakLE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_text_to_file(texts,filename):\n",
        "    with open(filename,'w') as outf:\n",
        "        outf.write('\\n'.join(texts))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YYl_XBBqkLE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TOKEN_RE = re.compile(r\"[a-zA-Z]+\")\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_text_simple_regex(text,min_token_size=0):\n",
        "    text = text.lower()\n",
        "    text = text.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
        "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
        "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
        "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
        "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
        "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
        "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"'d\",\" would\")\n",
        "    text = re.sub(r\"([0-9]+)000000\", r\"\\1m\", text)\n",
        "    text = re.sub(r\"([0-9]+)000\", r\"\\1k\", text)\n",
        "    \n",
        "    all_tokens = TOKEN_RE.findall(text)\n",
        "    return [token for token in all_tokens if len(token)>=min_token_size]\n",
        "    \n",
        "def tokenize_text(corpus,tokenizer=tokenize_text_simple_regex,**tokenizer_kwargs):\n",
        "    return [tokenizer(txt,**tokenizer_kwargs) for txt in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Jboaxy5LkLFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phrase_token = tokenize_text(phrases)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ah6uiHSJkLFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phrase_token = [ph for ph in phrase_token if len(ph)<30 and len(ph)>3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "FDnKn3rhkLFH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7dbf220c-1242-4305-8ec6-c244a8179809"
      },
      "source": [
        "new_phrases = [' '.join(send) for send in phrase_token]\n",
        "len(new_phrases)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "145655"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kXcKXObikLFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.shuffle(new_phrases)\n",
        "\n",
        "SPLIT = int(len(new_phrases) * 0.8)\n",
        "\n",
        "train_data = new_phrases[:SPLIT]\n",
        "test_data = new_phrases[SPLIT:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nk_OcFf7kLFQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "72f825d0-e06f-4336-94a3-bcc5612f7e7d"
      },
      "source": [
        "print(\"Size of train dataset: {}\".format(len(train_data)))\n",
        "print(\"Size of test dataset: {}\".format(len(test_data)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of train dataset: 116524\n",
            "Size of test dataset: 29131\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEKgPHpDkLFU",
        "colab_type": "text"
      },
      "source": [
        "### Tokenize text with BPE and create dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Kj6gSBgCkLFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_text_to_file(texts,filename):\n",
        "    with open(filename,'w') as outf:\n",
        "        outf.write('\\n'.join(texts))     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZMXnju_mkLFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FILE_BPE = 'quotes.yttm'\n",
        "TRAIN_TEXT_FILENAME = 'train_quotes.txt'\n",
        "save_text_to_file(train_data,TRAIN_TEXT_FILENAME)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "48qje7iqkLFX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cb62782-83ec-4c28-bd53-c5b9ad72b09c"
      },
      "source": [
        "yttm.BPE.train(data = TRAIN_TEXT_FILENAME,vocab_size = 2000,model = FILE_BPE )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<youtokentome.youtokentome.BPE at 0x7fe8bfe5d160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6Dq7goQykLFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = yttm.BPE(FILE_BPE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YSuioEFekLFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_token_ids = tokenizer.encode(train_data, bos = True, eos = True)\n",
        "test_token_ids = tokenizer.encode(test_data, bos = True, eos = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DAPwV9-0kLFf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81a04f32-e4bd-4489-aeb3-5e2134b5eb68"
      },
      "source": [
        "unknown_subwords_in_test = sum(1 for text in test_token_ids for token_id in text if token_id == 1)\n",
        "print('Number of cases with unknown n-grams of characters in the validation sample', unknown_subwords_in_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of cases with unknown n-grams of characters in the validation sample 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wmMiXScvkLFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ensure_lenght(txt, length,pad_value):\n",
        "    if(length>len(txt)):\n",
        "        return list(txt) + [pad_value] * (length - len(txt))\n",
        "    else:\n",
        "        return txt[:length]\n",
        "    \n",
        "class LanguageModelDataset(Dataset):\n",
        "    def __init__(self,token_ids, chunk_length = 30,pad_value = 0):\n",
        "        self.token_ids = token_ids\n",
        "        self.chunk_length = chunk_length\n",
        "        self.pad_value = pad_value\n",
        "    def __len__(self):\n",
        "        return len(self.token_ids)\n",
        "    def __getitem__(self,item):\n",
        "        \n",
        "        text = self.token_ids[item]\n",
        "        \n",
        "        seed_part = text[1:-1]\n",
        "        target_part = text[2:]\n",
        "        \n",
        "        seed_part = np.array(ensure_lenght(seed_part,self.chunk_length,self.pad_value))\n",
        "        target_part = np.array(ensure_lenght(target_part,self.chunk_length,self.pad_value))\n",
        "        \n",
        "        return seed_part,target_part"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ipNUYZ-LkLFi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = LanguageModelDataset(train_token_ids, 50,pad_value = 0)\n",
        "test_dataset =  LanguageModelDataset(test_token_ids, 50,pad_value = 0 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "caLRZ2hwkLFj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "fcab5e64-ae2e-4a6a-add7-ed6760b69347"
      },
      "source": [
        "tokenizer.decode(list(train_dataset[2]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['why fear death it is the most beautiful adventure in life<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>',\n",
              " 'fear death it is the most beautiful adventure in life<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n5ya3Ben1hs",
        "colab_type": "text"
      },
      "source": [
        "### Model and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zdxc1fS4kLFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_dependency_target_mask(size):\n",
        "    full_mask = torch.ones((size,size))\n",
        "    ignore_mask = torch.tril(full_mask)<1\n",
        "    full_mask.masked_fill_(ignore_mask,float('-inf'))\n",
        "    full_mask.masked_fill_(~ignore_mask,0)\n",
        "    return full_mask\n",
        "                    \n",
        "def make_positional_encoding(max_len,emb_size):\n",
        "    time = np.pi * torch.arange(0,max_len).float()\n",
        "    freq_dividers = torch.arange(1,emb_size//2+1)\n",
        "    inputs = time[:, None] / freq_dividers[None, :]\n",
        "    \n",
        "    result = torch.zeros(max_len,emb_size)\n",
        "    result[:, 0::2] = torch.sin(inputs)\n",
        "    result[:, 1::2] = torch.cos(inputs)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iWaP0wNJkLFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self,vocab_size,emb_size,backbone,emb_dropout = 0.0):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_size = emb_size\n",
        "        self.embeddings = nn.Embedding(vocab_size,emb_size,padding_idx = 0)\n",
        "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
        "        self.backbone = backbone\n",
        "        self.out = nn.Linear(emb_size,vocab_size)\n",
        "        \n",
        "    def forward(self,seed_token_ids):\n",
        "        batch_size,max_in_len = seed_token_ids.shape\n",
        "        \n",
        "        seed_padding_mask = seed_token_ids == 0\n",
        "        \n",
        "        dependency_mask = make_dependency_target_mask(max_in_len).to(seed_token_ids.device)\n",
        "        \n",
        "        seed_emb = self.embeddings(seed_token_ids)\n",
        "        \n",
        "        pos_codes = make_positional_encoding(max_in_len,self.emb_size).unsqueeze(0).to(seed_token_ids.device)\n",
        "        \n",
        "        seed_emb = seed_emb + pos_codes\n",
        "        seed_emb = self.emb_dropout(seed_emb)\n",
        "        \n",
        "        target_features = seed_emb\n",
        "        target_features = self.backbone(seed_emb, mask = dependency_mask, src_key_padding_mask = seed_padding_mask)\n",
        "        logits = self.out(target_features)\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5UUBC--AkLFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lm_cross_entropy(pred,target):\n",
        "    \"\"\"\n",
        "    pred - BatchSize x TargetLen x VocabSize\n",
        "    target - BatchSize x TargetLen\n",
        "    \"\"\"\n",
        "    pred_flat = pred.view(-1,pred.shape[-1])\n",
        "    target_flat = target.view(-1)\n",
        "    return F.cross_entropy(pred_flat,target_flat,ignore_index = 0)\n",
        "\n",
        "def lr_scheduler(optimizer):\n",
        "    return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 20, factor = 0.5, verbose = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fTN-BUGNkLFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchFirstTranformerEncoder(nn.Module):\n",
        "    def __init__(self,*args,**kwargs):\n",
        "        super().__init__()\n",
        "        self.impl = nn.TransformerEncoder(*args,**kwargs)\n",
        "        self.initialize_weights()\n",
        "    def forward(self,src,*args,**kwargs):\n",
        "        src = src.transpose(0,1).contiguous()\n",
        "        result = self.impl(src,*args,**kwargs)\n",
        "        result = result.transpose(0,1).contiguous()\n",
        "        return result\n",
        "    def initialize_weights(self):\n",
        "        for param in self.impl.parameters():\n",
        "            if param.dim() > 1:\n",
        "                nn.init.xavier_uniform_(param)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bK_x6paekLFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LanguageModel(tokenizer.vocab_size(),128,BatchFirstTranformerEncoder(nn.TransformerEncoderLayer(d_model = 128,\n",
        "                                                                                             nhead = 8,\n",
        "                                                                                             dim_feedforward=512,\n",
        "                                                                                             dropout=0.2),num_layers = 2),emb_dropout = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oJ4F0HyRkLFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(best_val_loss,\n",
        " best_torch_transf_model) = train_eval_loop(model,\n",
        "                                            train_dataset,\n",
        "                                            test_dataset,\n",
        "                                            lm_cross_entropy,\n",
        "                                            lr=2e-3,\n",
        "                                            epoch_n=700,\n",
        "                                            batch_size=1024,\n",
        "                                            device='cuda',\n",
        "                                            early_stopping_patience=30,\n",
        "                                            max_batches_per_epoch_train=1000,\n",
        "                                            max_batches_per_epoch_val=1000,\n",
        "                                            lr_scheduler_ctor=lr_scheduler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "71R8NQcekLFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(best_torch_transf_model.state_dict(), 'quotes_model.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xf_1vn5_kLFt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3eade6c2-cfbf-48e5-b41c-64e789b243d9"
      },
      "source": [
        "model.load_state_dict(torch.load('quotes_model.pth'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sawJJt9Rzgg0",
        "colab_type": "text"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3sas4NKckLFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomGeneration():\n",
        "    def __init__(self,model,tokenizer,device = 'cuda',eos_token_id = 3):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.eos_token_id = eos_token_id\n",
        "        self.device = torch.device(device)\n",
        "        self.model.to(self.device)\n",
        "        \n",
        "    def __call__(self,seed_phrase = '  ', temperature = 1.0,max_steps_n = 1000):\n",
        "        seed_tokens = tokenizer.encode(list([seed_phrase]))[0]\n",
        "        vocab_size = (self.tokenizer.vocab_size())\n",
        "                      \n",
        "        if len (seed_tokens) == 0:\n",
        "            seed_tokens.append(2)\n",
        "                      \n",
        "        for _ in range(max_steps_n):\n",
        "            in_batch = torch.tensor(seed_tokens).unsqueeze(0).to(self.device)\n",
        "            logits = self.model(in_batch)[0,-1]\n",
        "            logproba = F.softmax(logits/temperature).cpu().detach().numpy()\n",
        "            \n",
        "            next_token = np.random.choice(vocab_size,p = logproba)\n",
        "            seed_tokens.append(next_token)\n",
        "            \n",
        "            if(next_token == self.eos_token_id): break\n",
        "        \n",
        "        return self.tokenizer.decode([seed_tokens])[0][:-5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TpBjJowCkLFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generation = RandomGeneration(model,tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eCWktND0kLF0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "027db428-a26f-415e-b4f1-20e5a18831af"
      },
      "source": [
        "for _ in range(10):\n",
        "    print(generation(seed_phrase = 'power',temperature = 0.7))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "power is not the answer of your rewards but the question is how you treat them\n",
            "power is the most important thing you can do in life\n",
            "power is a delusion of change\n",
            "power makes you feel good\n",
            "power and power are the reflection of the world\n",
            "power is strength of a dangerous silence\n",
            "power is a genius so people can be a horse\n",
            "power is the process of time\n",
            "power is a choice it is a choice that is not a choice we make it happen to it\n",
            "power is disposition is the essence of an illusion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QX-NFq_JkLF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomGenerationStepTemp():\n",
        "    def __init__(self,model,tokenizer,device = 'cuda',eos_token_id = 3):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.eos_token_id = eos_token_id\n",
        "        self.device = torch.device(device)\n",
        "        self.model.to(self.device)\n",
        "        \n",
        "    def __call__(self,seed_phrase = '  ', temperature = 1.0,size_step = 10,step = 0.1,max_steps_n = 100):\n",
        "        seed_tokens = tokenizer.encode(list([seed_phrase]))[0]\n",
        "        vocab_size = (self.tokenizer.vocab_size())\n",
        "                      \n",
        "        if len (seed_tokens) == 0:\n",
        "            seed_tokens.append(2)\n",
        "                      \n",
        "        for i in range(max_steps_n):\n",
        "            in_batch = torch.tensor(seed_tokens).unsqueeze(0).to(self.device)\n",
        "            logits = self.model(in_batch)[0,-1]\n",
        "            logproba = F.softmax(logits/temperature).cpu().detach().numpy()\n",
        "            \n",
        "            next_token = np.random.choice(vocab_size,p = logproba)\n",
        "            seed_tokens.append(next_token)\n",
        "            if(i%size_step == 0):\n",
        "                temperature = temperature - step\n",
        "                if(temperature<=0):temperature = 0.1\n",
        "            \n",
        "            if(next_token == self.eos_token_id): break\n",
        "        \n",
        "        return self.tokenizer.decode([seed_tokens])[0][:-5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "i1VSyFu8kLF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generation_step = RandomGenerationStepTemp(model,tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4Vuoh5jckLF4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b514a909-82bc-48f8-9645-bc43b38dc02b"
      },
      "source": [
        "for _ in range(5):\n",
        "    print(generation_step(seed_phrase = 'artificial intelligence is',temperature = 0.8,size_step = 1,step = 0.05))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "artificial intelligence is the highest form of spiritual creation\n",
            "artificial intelligence is in the logic of adjustment\n",
            "artificial intelligence is the opposite of the human mind\n",
            "artificial intelligence is the most powerful force that you can see\n",
            "artificial intelligence is the only thing that can be realized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1QhEdieRkLF5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "58ee35c3-adab-4110-91fa-f79e98635746"
      },
      "source": [
        "for _ in range(5):\n",
        "    print(generation_step(seed_phrase = 'freedom is',temperature = 0.7,size_step = 1,step = 0.05))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "freedom is not a form of failure but a lack of courage\n",
            "freedom is the root of the soul\n",
            "freedom is the greatest gift of life\n",
            "freedom is not the most important thing that is the greatest thing to do is to do\n",
            "freedom is a process of finding out what you want to do\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSrVbJg2oKA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}